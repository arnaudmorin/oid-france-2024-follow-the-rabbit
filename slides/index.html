<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

  <title>Follow the RabbitMQ</title>

    <link rel="stylesheet" href="dist/reset.css">
    <link rel="stylesheet" href="dist/reveal.css">
    <link rel="stylesheet" href="dist/theme/solarized.css">

    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href="plugin/highlight/monokai.css">
  </head>
  <body>
    <div class="reveal">
      <div class="slides">

        <section>Follow the rabbit</section>

        <!-- Intro -->
        <section>
          <section>Who are we ?</section>
        </section>

        <section>
          <section>What do we do?</section>
        </section>

        <!-- Agenda of issues -->
        <section data-markdown>
            ## Agenda

            1. Who framed RabbitMQ?
            1. Troubleshoot in Wonderland
            1. Under the hood: oslo.messaging
            1. Follow the *right* rabbit
            1. Lessons learned from this journey
        </section>


        <!-- Issues -->
        <section data-markdown data-background-image='images/roger.gif'>
          # 1. Who framed RabbitMQ?
        </section>
        <section data-markdown
          data-separator="^\n---\n$"
          data-separator-vertical="^\n--\n$">
          <textarea data-template>
            ## Cluster unresponsive
            
            RabbitMQ server and/or cluster is unresponsive
            - stuck rabbitmqctl / api
            - a lot of error in rabbitmq logs
            - no more data from prom exporter

            --

            ## Missed heartbeats

            A lot of connection closed because of missed heartbeats
            - not something new
              - seems accepted by the community

            ## oslo.messaging RPC timeout

            A lot of oslo.messaging RPC timeout in all openstack component logs

            ## Network partition

            RabbitMQ cluster partition without any visible reason

            --

            ## Our (dirty) solution

            Reset cluster

            --

            ## Bad effects

            - events loss ➜ inconsistencies on openstack
            - loss 1 node ➜ non HA queue are lost (*transient* queues)

            --

            ## You said large-scale?

            Most of our region are bigger than 1k computes nodes

            Some regions have more than 2k computes nodes.


## troubleshoot

            - compute nodes: > 1000  (up to ~2000)
            - separated rabbit clusters for Nova/Neutron
            - RabbitMQ neutron cluster more affected than others

            --

            ## Neutron is the bad guy
            Each compute is having at least 4 neutron agents
            * neutron bgp agent
            * neutron vrack agent
            * neutron l3 agent
            * neutron metadata agent
            * neutron dhcp agent (eventually)

            ## Neutron is the bad guy
            Neutron create a lot of queues and connections!

            - 1 agent = 14 tcp connections on rabbit
            - 1 agent = at least N queues
              - Most of them are *transient* queues

            - Rolling update redeclare tons of queues / tcp connections on message broker
              - anormal cpu usage

            Note:

            120k queues on GRA7: thanos query screenshot to illustrate

            --

            #### Root cause ?

            - Wrong cluster sizing ?
              - Nova cluster is ok, and it's usage in nb msg/s if by far > neutron
              - But Neutron have more tcp connections / queues

            - Wrong RabbitMQ configuration ?

            - Openstack issue ?

          </textarea>
        </section>

        <!-- Troubleshoot -->
        <section data-markdown data-background-image='images/alice.gif'>
          # 2. Troubleshoot
        </section>
        <section data-markdown
          data-separator="^\n---\n$"
          data-separator-vertical="^\n--\n$">
          <textarea data-template>
            ## Better observability

            - Prometheus exporter + grafana dashboards
            - oslo.metrics
            - smokeping between cluster nodes

            Note:
            - screenshot of tuned grafana dashboard  + oslo.metrics
            - talk about patch we introduced in oslo.metrics

            --

            ## Take a look at logs

            *that sounds obvious*

            --

            ## Add some useful logs

            Add some logs with:
            - message ID
            - consumer ID
            - RPC called
            - etc.

            --

            ## Talk with upstream community

            *that sounds also obvious*

            --

            ## Better tooling

            - rabbitmqctl
            - rabbitmq-queues
            - rabbitmq-diagnostics
            - etc.

            Note:
            - rabbitmq-diagnostic observer is useful to know which erlang function is consuming cpu
            - a tons of events in a rabbit cluster is not normal

            --

            ## Reproduce production workload on dev

            - RabbitMQ perftest

            ```bash
             ❭ docker run -it --net=host --rm pivotalrabbitmq/perf-test:latest -H amqp://rabbit:xxx@10.69.212.68 \
                  --size 5000 --json-body \
                  --publishing-interval 5 \
                  --consumers 1000 \
                  --producers 3000 \
                  -f persistent \
                  -queue-pattern "ha_queue_%s" \
                  --queue-pattern-from 1 \
                  --queue-pattern-to 30000
            ```

            --

            ## What we learned?

            - oslo.metrics: most of messages in neutron are sent in fanouts
              - 1 message published, replicated to n queues
              - neutron-server flood resources update to agents (agent notifier api, resource cache)

            --

            ## What we learned?

            - perftest:
              - rabbitmq does not like at all handling tons of internal events (connection, channel, queue create/delete, etc..)
              - it's designed to handle high msg/s
              - it's a message broker, not a queue/connection broker!

            --

            ## What we learned?

            - grafana:
              - bottleneck on file descriptor usage
                - a persistent queue use 1 fd stored on disk
          </textarea>
        </section>


        <!-- Under the hood -->
        <section data-markdown data-background-image='images/matrix.gif'>
          # 3. Under the hood: oslo.messaging
        </section>
        <section data-markdown
          data-separator="^\n---\n$"
          data-separator-vertical="^\n--\n$">
          <textarea data-template>
            ## Pub/Sub mechanism

            Note:
            Schema:
            publish to an exchange with a routing key
            routed to 1 or N queue

            1 queue => 1 consumer (except for stream, we'll see that later)

            --

            ## RPC server

            - RPCServer() initialisation
              - a topic name
              - 1 or more endpoint class with rpc methods

            Note:
            1 rpcserver = 1 tcp connection
            listening on 3 queues
              topic
              topic.hostname
              topic_fanout

            --

            ## Policy and queues

            Recommended HA policy:

              `^(?!(amq\.)|(.*_fanout_)|(reply_)).*`

            * No HA on *transient* queues:
              * reply\_[rand]
              * \*\_fanout\_[rand]

            --

            ## Publisher

            - methods:
              - call
                - reply queue (transient queue)
              - cast
              - cast fanout=True

            - Ex of a rpc call:

            '''shell
            here
            ```

          </textarea>
        </section>



    <!-- Journey to get a stable infra. -->
    <!-- -->
    <!--     Infra improvment -->
    <!--         split rabbit-neutron / rabbit-* -->
    <!--         scale problematic clusters to 5 node -->
    <!--         Upgrade to 3.10+ -->
    <!--             quorum queue recommended -->
    <!--         put back partition strategy to pause-minority -->
    <!-- -->
    <!--     oslo messaging improvments -->
    <!--         queue fixed naming to avoid queue churn -->
    <!--         heartbeat in pthread fix -->
    <!--         move from HA queue > Quorum queues -->
    <!--             fix to autodelete broken quorum queues -->
    <!--         replace 'fanout' queues by stream queues -->
    <!--             reduce queue nb a lot -->
    <!--             patch to avoid tcp reconnection when a queue is deleted (kombu/oslo) -->
    <!--         reduce queues declared by a RPC server (3 queues by default to only 1) -->
    <!--         use same connection for mutiple topics -->


        <!-- Solutions -->
        <section data-markdown data-background-image='images/matrixlapin.jpeg'>
          # 4. Follow the *right* rabbit
        </section>
        <section data-markdown
          data-separator="^\n---\n$"
          data-separator-vertical="^\n--\n$">
          <textarea data-template>
            ## RabbitMQ upgrade

            3.8 ➜ 3.12

            --

            Better, but not enough

            --

            ## Avoid missed heartbeats

            A lot of connections are closed because of missed heartbeats

            * not something new
            * seems accepted by the community

            Let's fix it anyway

            --

            ## Switch apache mpm

            worker ➜ event

            --

            Better, but not enough

            --

            ## Avoid missed heartbeats on compute nodes

            * fixed green threads (some fix were already done by RedHat)
            * fixed pyamqp (timeout not respected)

            --

            Better, but not enough

            --

            ## Introducing RPC pinger

            A new endpoint to help operators monitoring agents liveness


            ## Configuring RPC pinger

            ```shell
            [oslo.messaging]
            rpc_ping_enabled = True
            ```

            + tool that actually send call RPC ping on all agents every 5 minutes

            --

            ## Good results

            Very precise monitoring of which neutron / nova agent is dead or not

            --

            ## Bad results

            Too much messages sent over RabbitMQ wires (x10)

            Not as reliable as expected:
            * each RPC server run in a separate thread

            --

            Better, but not enough

            --
            
            ## Support a node loss

            * Recommended policy is not HA for all queues
              * node loss ➜ queues loss ➜ messages loss
            * Classic HA are deprecated

            --

            ## Introducing quorum queues

            * Quorum are replacing classic HA
            * RabbitMQ community recommands quorum

            * oslo.messaging partial support
              * missing support for *transient* queues

            --

            ## Configuring quorum queues

            ```shell
            [oslo.messaging]
            rabbit_quorum_queue = True
            rabbit_transient_quorum_queue = True
            ```

            --

            ## Good results

            All queues are now HA

            We support a node loss!

            --

            Better, but not enough

            --

            ## Bad results

            * more CPU
            * more network
            * more RAM

            --

            ## Very bad results

            Queue churn is killing servers/cluster

            * Rabbit unresponsive / slow
            * Cluster partition

            --

            Better, but not enough

            --

            ## Avoid queue churn

            Queue churn is due to randomness in *transient* queues
            * reply\_[rand]
            * \*\_fanout\_[rand]

            --

            ## Introducing queue manager
            ```shell
            [oslo.messaging]
            use_queue_manager = True
            hostname = yourhostname
            processname = yourprocessname
            ```

            ## Good results

            reply\_[rand] ➜ reply_host1234:neutron-bgp-agent:1

            On service restart: **no queue churn**

            --

            Better, but not enough

            --

            ## Too much queue issue

            Too much queues on RabbitMQ

            **Side effect**:
            * Queue leader re-election is costly
              * Can kill the cluster

            --

            ## Introducing stream queues

            As RabbitMQ doc say:

            Use Cases for Using Streams: Large fan-outs
            Goal here is to replace topic_fanout queues (1 pub to n consumers)

            ```shell
            [oslo.messaging]
            rabbit_stream_fanout = True
            ```

            Note:
            In rabbitmq, each consumer declare it's own queue.
            with stream, same queue can be shared with N consumers.

            --

            ## Stream queue

            Add a schema, with N consumer consuming at a specific offset

            Add a screenshot of nb queues before/after

            --

            ## Good results

            Reduce the number of queues, from 128k to 70k

            --

            Better, but not enough

            --

            ## Stop creating unused queues

            Neutron creates a lot of queues, while it seems they are not used!

            Can we delete them / stop creating them?

            --

            ## Cleanup up neutron / oslo.messaging code

            Each RPC server declare 3 queues (like we explained before)

            status: not submitted yet upstream

            Note:

            Neutron still has too many queues

          </textarea>
        </section>

        <!-- What we learned from that journey -->
        <section data-markdown
          data-separator="^\n---\n$"
          data-separator-vertical="^\n--\n$">
          <textarea data-template>
            ### Lessons learned
            TODO
          </textarea>
        </section>



      </div>
    </div>

    <script src="dist/reveal.js"></script>
    <script src="plugin/notes/notes.js"></script>
    <script src="plugin/markdown/markdown.js"></script>
    <script src="plugin/highlight/highlight.js"></script>
    <script>
    // More info about initialization & config:
    // - https://revealjs.com/initialization/
    // - https://revealjs.com/config/
    Reveal.initialize({
      hash: true,

      // Learn about plugins: https://revealjs.com/plugins/
      plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
    });
    </script>
  </body>
</html>
